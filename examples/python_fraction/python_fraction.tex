\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{natbib}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newcommand{\sdag}[1]{{#1}^{\dag}}

\title{Fraction of Time MPI-SPPY spends in Python}
\author{ David L Woodruff\\
  Graduate School of Management\\
  \\
  University of California Davis\\
  Davis CA 95616 USA}
\date{\today}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}
\maketitle

When considering applications in practice, or when comparing to other
packages, a question arises concerning the fraction of time that
mpi-sppy spends ``in Python'' as opposed to compiled code written in
other languages such as C and Fortran.  Since solvers, numpy, and MPI
are all in the latter category, {\em a priori} one expects that the
fraction spent in Python will be small for all but toy problems.

We have use the tool called {\em scalene}, which is designed to
attribute time to Python vs native (it can estimate time spent in
compiled code called from Python, which it calls ``native''). It works
by sampling.

It turns out that about 10\% to somtimes 20\% of
MPI-SPPY's time is spent in Python code. So you can't get much speed-up just
using another language. Maybe another language will make it easier
for you to do something sophisticated and algorithmic.

NOTE: I don't yet understand why, based on very limited samples, the fraction of
time spent in python seems to be larger in ranks above the first three.

\appendix
\section{Dec 2025}

These experiments use Pyomo models and the Pyomo time is included. 

% Auto-generated by make_scalene_latex_table.py

\noindent\textbf{System information:}\\
\begin{itemize}
  \item \texttt{os}={Linux 6.14.0-37-generic (x86\_64)}
  \item \texttt{cpu\_model}={Intel(R) Core(TM) i7-14700}
  \item \texttt{cpu\_logical}={28}
  \item \texttt{cpu\_physical\_cores}={20}
  \item \texttt{cpu\_mhz}={5400.0000}
  \item \texttt{cpu\_sockets}={1}
  \item \texttt{cores\_per\_socket}={20}
  \item \texttt{threads\_per\_core}={2}
  \item \texttt{mem\_total}={31.03 GiB}
  \item \texttt{mem\_available}={25.16 GiB}
\end{itemize}

% Run parameters extracted from JSON (best-effort):
% argv: ../../mpisppy/generic\_cylinders.py --module-name ../farmer/farmer --num-scens 3 --solver-name gurobi --max-iterations 100 --max-solver-threads 4 --default-rho 1 --lagrangian --xhatshuffle --rel-gap 0.0001
% Parsed parameters:
%   default-rho: 1
%   lagrangian: true
%   max-iterations: 100
%   max-solver-threads: 4
%   module-name: ../farmer/farmer
%   num-scens: 3
%   rel-gap: 0.0001
%   solver-name: gurobi
%   target: ../../mpisppy/generic\_cylinders.py
%   xhatshuffle: true

\noindent\textbf{Run parameters (from Scalene JSON):}\\
\begin{itemize}
  \item \texttt{target}={../../mpisppy/generic\_cylinders.py}
  \item \texttt{module-name}={../farmer/farmer}
  \item \texttt{num-scens}={3}
  \item \texttt{solver-name}={gurobi}
  \item \texttt{max-iterations}={100}
  \item \texttt{max-solver-threads}={4}
  \item \texttt{default-rho}={1}
  \item \texttt{rel-gap}={0.0001}
  \item \texttt{lagrangian}={true}
  \item \texttt{xhatshuffle}={true}
\end{itemize}

\begin{table}[ht]
\centering
\begin{tabular}{r l r r r r}
\hline
Rank & File & Wall (s) & Python (s) & Native (s) & System (s) \\
\hline
0 & scalene\_rank\_0.json & 1.90 & 0.17 & 1.54 & 0.11 \\
1 & scalene\_rank\_1.json & 1.90 & 0.19 & 1.52 & 0.08 \\
2 & scalene\_rank\_2.json & 1.84 & 0.17 & 1.53 & 0.11 \\
\hline
\end{tabular}
\caption{Scalene timing summary by MPI rank}
\label{tab:scalene-summary}
\end{table}

% Auto-generated by make_scalene_latex_table.py

\noindent\textbf{System information:}\\
\begin{itemize}
  \item \texttt{os}={Linux 6.14.0-37-generic (x86\_64)}
  \item \texttt{cpu\_model}={Intel(R) Core(TM) i7-14700}
  \item \texttt{cpu\_logical}={28}
  \item \texttt{cpu\_physical\_cores}={20}
  \item \texttt{cpu\_mhz}={5400.0000}
  \item \texttt{cpu\_sockets}={1}
  \item \texttt{cores\_per\_socket}={20}
  \item \texttt{threads\_per\_core}={2}
  \item \texttt{mem\_total}={31.03 GiB}
  \item \texttt{mem\_available}={24.79 GiB}
\end{itemize}

% Run parameters extracted from JSON (best-effort):
% argv: ../../mpisppy/generic\_cylinders.py --module-name ../sslp/sslp --sslp-data-path ../sslp/data --instance-name sslp\_15\_45\_10 --solver-name gurobi --max-iterations 10 --max-solver-threads 4 --default-rho 1 --lagrangian --xhatshuffle --rel-gap 0.01
% Parsed parameters:
%   default-rho: 1
%   lagrangian: true
%   max-iterations: 10
%   max-solver-threads: 4
%   module-name: ../sslp/sslp
%   rel-gap: 0.01
%   solver-name: gurobi
%   target: ../../mpisppy/generic\_cylinders.py
%   xhatshuffle: true

\noindent\textbf{Run parameters (from Scalene JSON):}\\
\begin{itemize}
  \item \texttt{target}={../../mpisppy/generic\_cylinders.py}
  \item \texttt{module-name}={../sslp/sslp}
  \item \texttt{solver-name}={gurobi}
  \item \texttt{max-iterations}={10}
  \item \texttt{max-solver-threads}={4}
  \item \texttt{default-rho}={1}
  \item \texttt{rel-gap}={0.01}
  \item \texttt{lagrangian}={true}
  \item \texttt{xhatshuffle}={true}
\end{itemize}

\begin{table}[ht]
\centering
\begin{tabular}{r l r r r r}
\hline
Rank & File & Wall (s) & Python (s) & Native (s) & System (s) \\
\hline
0 & scalene\_rank\_0.json & 13.83 & 1.24 & 11.20 & 0.83 \\
1 & scalene\_rank\_1.json & 13.82 & 1.38 & 11.06 & 0.55 \\
2 & scalene\_rank\_2.json & 13.82 & 1.24 & 11.47 & 0.83 \\
\hline
\end{tabular}
\caption{Scalene timing summary by MPI rank}
\label{tab:scalene-summary}
\end{table}

This table shows sslp with more ranks:

% Auto-generated by make_scalene_latex_table.py

\noindent\textbf{System information:}\\
\begin{itemize}
  \item \texttt{os}={Linux 6.14.0-37-generic (x86\_64)}
  \item \texttt{cpu\_model}={Intel(R) Core(TM) i7-14700}
  \item \texttt{cpu\_logical}={28}
  \item \texttt{cpu\_physical\_cores}={20}
  \item \texttt{cpu\_mhz}={5400.0000}
  \item \texttt{cpu\_sockets}={1}
  \item \texttt{cores\_per\_socket}={20}
  \item \texttt{threads\_per\_core}={2}
  \item \texttt{mem\_total}={31.03 GiB}
  \item \texttt{mem\_available}={24.83 GiB}
\end{itemize}

% Run parameters extracted from JSON (best-effort):
% argv: ../../mpisppy/generic\_cylinders.py --module-name ../sslp/sslp --sslp-data-path ../sslp/data --instance-name sslp\_15\_45\_10 --solver-name gurobi --max-iterations 10 --max-solver-threads 4 --default-rho 1 --lagrangian --xhatshuffle --rel-gap 0.01
% Parsed parameters:
%   default-rho: 1
%   lagrangian: true
%   max-iterations: 10
%   max-solver-threads: 4
%   module-name: ../sslp/sslp
%   rel-gap: 0.01
%   solver-name: gurobi
%   target: ../../mpisppy/generic\_cylinders.py
%   xhatshuffle: true

\noindent\textbf{Run parameters (from Scalene JSON):}\\
\begin{itemize}
  \item \texttt{target}={../../mpisppy/generic\_cylinders.py}
  \item \texttt{module-name}={../sslp/sslp}
  \item \texttt{solver-name}={gurobi}
  \item \texttt{max-iterations}={10}
  \item \texttt{max-solver-threads}={4}
  \item \texttt{default-rho}={1}
  \item \texttt{rel-gap}={0.01}
  \item \texttt{lagrangian}={true}
  \item \texttt{xhatshuffle}={true}
\end{itemize}

\begin{table}[ht]
\centering
\begin{tabular}{r l r r r r}
\hline
Rank & File & Wall (s) & Python (s) & Native (s) & System (s) \\
\hline
0 & scalene\_rank\_0.json & 9.89 & 0.89 & 8.01 & 0.59 \\
1 & scalene\_rank\_1.json & 9.89 & 0.99 & 7.91 & 0.40 \\
2 & scalene\_rank\_2.json & 9.93 & 0.89 & 8.24 & 0.60 \\
3 & scalene\_rank\_3.json & 9.89 & 1.88 & 7.02 & 0.00 \\
4 & scalene\_rank\_4.json & 9.89 & 1.48 & 7.52 & 0.10 \\
5 & scalene\_rank\_5.json & 9.89 & 1.58 & 7.22 & 0.10 \\
\hline
\end{tabular}
\caption{Scalene timing summary by MPI rank}
\label{tab:scalene-summary}
\end{table}

And even more. 

% Auto-generated by make_scalene_latex_table.py

\noindent\textbf{System information:}\\
\begin{itemize}
  \item \texttt{os}={Linux 6.14.0-37-generic (x86\_64)}
  \item \texttt{cpu\_model}={Intel(R) Core(TM) i7-14700}
  \item \texttt{cpu\_logical}={28}
  \item \texttt{cpu\_physical\_cores}={20}
  \item \texttt{cpu\_mhz}={5400.0000}
  \item \texttt{cpu\_sockets}={1}
  \item \texttt{cores\_per\_socket}={20}
  \item \texttt{threads\_per\_core}={2}
  \item \texttt{mem\_total}={31.03 GiB}
  \item \texttt{mem\_available}={25.00 GiB}
\end{itemize}

% Run parameters extracted from JSON (best-effort):
% argv: ../../mpisppy/generic\_cylinders.py --module-name ../sslp/sslp --sslp-data-path ../sslp/data --instance-name sslp\_15\_45\_10 --solver-name gurobi --max-iterations 10 --max-solver-threads 4 --default-rho 1 --lagrangian --xhatshuffle --rel-gap 0.01
% Parsed parameters:
%   default-rho: 1
%   lagrangian: true
%   max-iterations: 10
%   max-solver-threads: 4
%   module-name: ../sslp/sslp
%   rel-gap: 0.01
%   solver-name: gurobi
%   target: ../../mpisppy/generic\_cylinders.py
%   xhatshuffle: true

\noindent\textbf{Run parameters (from Scalene JSON):}\\
\begin{itemize}
  \item \texttt{target}={../../mpisppy/generic\_cylinders.py}
  \item \texttt{module-name}={../sslp/sslp}
  \item \texttt{solver-name}={gurobi}
  \item \texttt{max-iterations}={10}
  \item \texttt{max-solver-threads}={4}
  \item \texttt{default-rho}={1}
  \item \texttt{rel-gap}={0.01}
  \item \texttt{lagrangian}={true}
  \item \texttt{xhatshuffle}={true}
\end{itemize}

\begin{table}[ht]
\centering
\begin{tabular}{r l r r r r}
\hline
Rank & File & Wall (s) & Python (s) & Native (s) & System (s) \\
\hline
0 & scalene\_rank\_0.json & 9.44 & 0.85 & 7.65 & 0.57 \\
1 & scalene\_rank\_1.json & 9.64 & 0.96 & 7.71 & 0.39 \\
2 & scalene\_rank\_2.json & 9.58 & 0.86 & 7.95 & 0.57 \\
3 & scalene\_rank\_3.json & 9.73 & 1.85 & 6.91 & 0.00 \\
4 & scalene\_rank\_4.json & 9.47 & 1.42 & 7.20 & 0.09 \\
5 & scalene\_rank\_5.json & 9.74 & 1.56 & 7.11 & 0.10 \\
6 & scalene\_rank\_6.json & 9.78 & 1.76 & 7.33 & 0.20 \\
7 & scalene\_rank\_7.json & 9.74 & 1.66 & 7.30 & 0.29 \\
8 & scalene\_rank\_8.json & 9.55 & 1.72 & 6.49 & 0.57 \\
9 & scalene\_rank\_9.json & 9.71 & 1.94 & 6.99 & 0.19 \\
10 & scalene\_rank\_10.json & 9.79 & 1.86 & 7.24 & 0.20 \\
11 & scalene\_rank\_11.json & 9.73 & 1.36 & 7.00 & 0.39 \\
\hline
\hline
\end{tabular}
\caption{Scalene timing summary by MPI rank}
\label{tab:scalene-summary}
\end{table}


\end{document}

scalene is designed to attribute time to Python vs native (it can estimate time spent in compiled code called from Python). It’s often the most straightforward way to get exactly what you asked.

Run:

python -m pip install scalene
python -m scalene your_script.py [args...]


It reports per-file and per-line:

Python time

Native time (extensions, libraries like NumPy, solver bindings, etc.)

MPI note: if you run under mpiexec, you’ll get output per rank (may need to direct to separate files):

mpiexec -np 4 python -m scalene --outfile scalene_rank_%r.txt your_script.py ...


If %r isn’t supported in your shell, just set unique outfile names using env vars per rank.

This is usually the quickest route to “fraction spent in Python.”        

